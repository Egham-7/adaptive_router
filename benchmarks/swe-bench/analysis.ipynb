{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWE-bench Verified - Adaptive Router Analysis\n",
    "\n",
    "Analysis of 500 instances from SWE-bench Verified benchmark run with Adaptive AI router.\n",
    "\n",
    "**Routing Models:** Claude Opus 4.5 + Claude Sonnet 4.5\n",
    "\n",
    "**Run ID:** adaptive_20251215_124758"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load generation results\n",
    "results_path = Path('results/adaptive/adaptive_20251215_124758_generation.json')\n",
    "with open(results_path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract summary info\n",
    "print(\"=\" * 60)\n",
    "print(\"SWE-bench Verified - Adaptive Router Results\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nModel: {data['model_name']}\")\n",
    "print(f\"Dataset: {data['dataset']}\")\n",
    "print(f\"Timestamp: {data['timestamp']}\")\n",
    "print(\"\\n\" + \"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = data['summary']\n",
    "cost = data['cost_metrics']\n",
    "tokens = data['token_metrics']\n",
    "\n",
    "print(\"\\nüìä SUMMARY STATISTICS\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total Instances:     {summary['total_instances']}\")\n",
    "print(f\"Patches Generated:   {summary['total_instances'] - summary['failed_instances'] - summary['error_instances']}\")\n",
    "print(f\"Failed Instances:    {summary['failed_instances']}\")\n",
    "print(f\"Error Instances:     {summary['error_instances']}\")\n",
    "\n",
    "print(\"\\nüí∞ COST METRICS\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total Cost:          ${cost['total_cost_usd']:.4f}\")\n",
    "print(f\"Cost per Instance:   ${cost['cost_per_instance']:.4f}\")\n",
    "\n",
    "print(\"\\nüî¢ TOKEN METRICS\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total Tokens:        {tokens['total_tokens']:,}\")\n",
    "print(f\"Input Tokens:        {tokens['total_input_tokens']:,}\")\n",
    "print(f\"Output Tokens:       {tokens['total_output_tokens']:,}\")\n",
    "print(f\"Avg Tokens/Instance: {tokens['total_tokens'] / summary['total_instances']:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create DataFrame from Instance Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "instances = data['instance_results']\n",
    "df = pd.DataFrame(instances)\n",
    "\n",
    "# Extract nested generation_metrics\n",
    "gen_metrics = pd.json_normalize(df['generation_metrics'])\n",
    "gen_metrics.columns = ['gen_' + col for col in gen_metrics.columns]\n",
    "df = pd.concat([df.drop('generation_metrics', axis=1), gen_metrics], axis=1)\n",
    "\n",
    "# Extract repo name from instance_id\n",
    "df['repo_name'] = df['instance_id'].apply(lambda x: x.split('__')[0].replace('_', '-'))\n",
    "\n",
    "# Display first few rows\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Selection Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection distribution\n",
    "model_counts = df['gen_model_used'].value_counts()\n",
    "\n",
    "print(\"\\nü§ñ MODEL SELECTION DISTRIBUTION\")\n",
    "print(\"-\" * 40)\n",
    "for model, count in model_counts.items():\n",
    "    pct = count / len(df) * 100\n",
    "    print(f\"{model}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "model_labels = [m.replace('anthropic/', '').replace('-20250929', '') for m in model_counts.index]\n",
    "axes[0].pie(model_counts.values, labels=model_labels, autopct='%1.1f%%', \n",
    "            colors=colors[:len(model_counts)], startangle=90)\n",
    "axes[0].set_title('Model Selection Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bar chart\n",
    "bars = axes[1].bar(model_labels, model_counts.values, color=colors[:len(model_counts)])\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Requests per Model', fontsize=14, fontweight='bold')\n",
    "for bar, count in zip(bars, model_counts.values):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, \n",
    "                 str(count), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_selection_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost by model\n",
    "cost_by_model = df.groupby('gen_model_used').agg({\n",
    "    'gen_cost_usd': ['sum', 'mean', 'std'],\n",
    "    'instance_id': 'count'\n",
    "}).round(4)\n",
    "cost_by_model.columns = ['total_cost', 'avg_cost', 'std_cost', 'count']\n",
    "\n",
    "print(\"\\nüí∞ COST BY MODEL\")\n",
    "print(\"-\" * 60)\n",
    "print(cost_by_model.to_string())\n",
    "\n",
    "# Cost distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Box plot of costs by model\n",
    "models = df['gen_model_used'].unique()\n",
    "model_labels_short = [m.replace('anthropic/', '').replace('-20250929', '') for m in models]\n",
    "data_for_box = [df[df['gen_model_used'] == m]['gen_cost_usd'] for m in models]\n",
    "bp = axes[0].boxplot(data_for_box, labels=model_labels_short, patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], colors[:len(models)]):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].set_ylabel('Cost per Request ($)')\n",
    "axes[0].set_title('Cost Distribution by Model', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Histogram of costs\n",
    "axes[1].hist(df['gen_cost_usd'], bins=50, edgecolor='black', alpha=0.7, color='#3498db')\n",
    "axes[1].axvline(df['gen_cost_usd'].mean(), color='red', linestyle='--', label=f'Mean: ${df[\"gen_cost_usd\"].mean():.4f}')\n",
    "axes[1].axvline(df['gen_cost_usd'].median(), color='orange', linestyle='--', label=f'Median: ${df[\"gen_cost_usd\"].median():.4f}')\n",
    "axes[1].set_xlabel('Cost ($)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Cost Distribution (All Instances)', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cost_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Token Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token statistics\n",
    "print(\"\\nüî¢ TOKEN STATISTICS\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Input Tokens  - Mean: {df['gen_input_tokens'].mean():.0f}, Median: {df['gen_input_tokens'].median():.0f}, Max: {df['gen_input_tokens'].max()}\")\n",
    "print(f\"Output Tokens - Mean: {df['gen_output_tokens'].mean():.0f}, Median: {df['gen_output_tokens'].median():.0f}, Max: {df['gen_output_tokens'].max()}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Input vs Output tokens scatter\n",
    "scatter = axes[0].scatter(df['gen_input_tokens'], df['gen_output_tokens'], \n",
    "                          c=df['gen_cost_usd'], cmap='viridis', alpha=0.6, s=30)\n",
    "axes[0].set_xlabel('Input Tokens')\n",
    "axes[0].set_ylabel('Output Tokens')\n",
    "axes[0].set_title('Input vs Output Tokens (colored by cost)', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter, ax=axes[0], label='Cost ($)')\n",
    "\n",
    "# Token distribution by model\n",
    "df['gen_total_tokens'] = df['gen_input_tokens'] + df['gen_output_tokens']\n",
    "token_by_model = df.groupby('gen_model_used')['gen_total_tokens'].mean()\n",
    "model_labels_short = [m.replace('anthropic/', '').replace('-20250929', '') for m in token_by_model.index]\n",
    "bars = axes[1].bar(model_labels_short, token_by_model.values, color=colors[:len(token_by_model)])\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].set_ylabel('Average Tokens')\n",
    "axes[1].set_title('Average Tokens per Model', fontsize=14, fontweight='bold')\n",
    "for bar, val in zip(bars, token_by_model.values):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10, \n",
    "                 f'{val:.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('token_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Latency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latency statistics\n",
    "print(\"\\n‚è±Ô∏è LATENCY STATISTICS\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Overall - Mean: {df['gen_latency_seconds'].mean():.2f}s, Median: {df['gen_latency_seconds'].median():.2f}s\")\n",
    "print(f\"Min: {df['gen_latency_seconds'].min():.2f}s, Max: {df['gen_latency_seconds'].max():.2f}s\")\n",
    "\n",
    "# By model\n",
    "latency_by_model = df.groupby('gen_model_used')['gen_latency_seconds'].agg(['mean', 'median', 'std']).round(2)\n",
    "print(\"\\nLatency by Model:\")\n",
    "print(latency_by_model.to_string())\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Box plot\n",
    "data_for_box = [df[df['gen_model_used'] == m]['gen_latency_seconds'] for m in models]\n",
    "bp = axes[0].boxplot(data_for_box, labels=model_labels_short, patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], colors[:len(models)]):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].set_ylabel('Latency (seconds)')\n",
    "axes[0].set_title('Latency Distribution by Model', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Histogram\n",
    "axes[1].hist(df['gen_latency_seconds'], bins=50, edgecolor='black', alpha=0.7, color='#9b59b6')\n",
    "axes[1].axvline(df['gen_latency_seconds'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {df[\"gen_latency_seconds\"].mean():.2f}s')\n",
    "axes[1].set_xlabel('Latency (seconds)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Latency Distribution (All Instances)', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('latency_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Repository Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instances by repository\n",
    "repo_counts = df['repo_name'].value_counts()\n",
    "\n",
    "print(\"\\nüìÅ INSTANCES BY REPOSITORY\")\n",
    "print(\"-\" * 60)\n",
    "for repo, count in repo_counts.items():\n",
    "    pct = count / len(df) * 100\n",
    "    print(f\"{repo}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Visualize top repositories\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "top_repos = repo_counts.head(10)\n",
    "bars = ax.barh(top_repos.index[::-1], top_repos.values[::-1], color='#3498db')\n",
    "ax.set_xlabel('Number of Instances')\n",
    "ax.set_title('Top 10 Repositories by Instance Count', fontsize=14, fontweight='bold')\n",
    "\n",
    "for bar, count in zip(bars, top_repos.values[::-1]):\n",
    "    ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2, \n",
    "            str(count), ha='left', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('repository_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Selection by Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tabulation of model selection by repo\n",
    "cross_tab = pd.crosstab(df['repo_name'], df['gen_model_used'], normalize='index') * 100\n",
    "cross_tab.columns = [c.replace('anthropic/', '').replace('-20250929', '') for c in cross_tab.columns]\n",
    "\n",
    "# Visualize for top repos\n",
    "top_repo_names = repo_counts.head(10).index\n",
    "cross_tab_top = cross_tab.loc[top_repo_names]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "cross_tab_top.plot(kind='barh', stacked=True, ax=ax, color=colors[:len(cross_tab_top.columns)])\n",
    "ax.set_xlabel('Percentage (%)')\n",
    "ax.set_title('Model Selection by Repository (Top 10)', fontsize=14, fontweight='bold')\n",
    "ax.legend(title='Model', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_selection_by_repo.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nModel Selection % by Repository:\")\n",
    "print(cross_tab_top.round(1).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cost Efficiency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cost per 1K tokens\n",
    "df['cost_per_1k_tokens'] = (df['gen_cost_usd'] / df['gen_total_tokens']) * 1000\n",
    "\n",
    "print(\"\\nüí∞ COST EFFICIENCY\")\n",
    "print(\"-\" * 60)\n",
    "efficiency = df.groupby('gen_model_used').agg({\n",
    "    'cost_per_1k_tokens': 'mean',\n",
    "    'gen_cost_usd': 'sum',\n",
    "    'gen_total_tokens': 'sum'\n",
    "})\n",
    "efficiency.columns = ['avg_cost_per_1k', 'total_cost', 'total_tokens']\n",
    "efficiency['overall_cost_per_1k'] = (efficiency['total_cost'] / efficiency['total_tokens']) * 1000\n",
    "print(efficiency.round(4).to_string())\n",
    "\n",
    "# Hypothetical comparison: What if we only used one model?\n",
    "print(\"\\nüìä HYPOTHETICAL COMPARISON\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Get actual costs\n",
    "actual_cost = df['gen_cost_usd'].sum()\n",
    "print(f\"Actual Total Cost (Adaptive Router): ${actual_cost:.4f}\")\n",
    "\n",
    "# Note: Hypothetical costs would require knowing the per-token pricing for each model\n",
    "# Since the router dynamically selects based on task, we can analyze the selection pattern\n",
    "print(f\"\\nInstances routed to each model:\")\n",
    "for model in df['gen_model_used'].unique():\n",
    "    count = len(df[df['gen_model_used'] == model])\n",
    "    cost = df[df['gen_model_used'] == model]['gen_cost_usd'].sum()\n",
    "    print(f\"  {model.replace('anthropic/', '')}: {count} instances, ${cost:.4f} total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary dashboard\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# 1. Model distribution pie\n",
    "ax1 = fig.add_subplot(2, 3, 1)\n",
    "model_labels = [m.replace('anthropic/', '').replace('-20250929', '') for m in model_counts.index]\n",
    "ax1.pie(model_counts.values, labels=model_labels, autopct='%1.1f%%', colors=colors[:len(model_counts)])\n",
    "ax1.set_title('Model Selection', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 2. Cost distribution\n",
    "ax2 = fig.add_subplot(2, 3, 2)\n",
    "ax2.hist(df['gen_cost_usd'], bins=30, edgecolor='black', alpha=0.7, color='#3498db')\n",
    "ax2.axvline(df['gen_cost_usd'].mean(), color='red', linestyle='--', label=f'Mean: ${df[\"gen_cost_usd\"].mean():.4f}')\n",
    "ax2.set_xlabel('Cost ($)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Cost Distribution', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=8)\n",
    "\n",
    "# 3. Latency distribution\n",
    "ax3 = fig.add_subplot(2, 3, 3)\n",
    "ax3.hist(df['gen_latency_seconds'], bins=30, edgecolor='black', alpha=0.7, color='#9b59b6')\n",
    "ax3.axvline(df['gen_latency_seconds'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"gen_latency_seconds\"].mean():.1f}s')\n",
    "ax3.set_xlabel('Latency (s)')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.set_title('Latency Distribution', fontsize=12, fontweight='bold')\n",
    "ax3.legend(fontsize=8)\n",
    "\n",
    "# 4. Repository distribution\n",
    "ax4 = fig.add_subplot(2, 3, 4)\n",
    "top_repos = repo_counts.head(5)\n",
    "ax4.barh(top_repos.index[::-1], top_repos.values[::-1], color='#2ecc71')\n",
    "ax4.set_xlabel('Count')\n",
    "ax4.set_title('Top 5 Repositories', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 5. Tokens by model\n",
    "ax5 = fig.add_subplot(2, 3, 5)\n",
    "token_means = df.groupby('gen_model_used')['gen_total_tokens'].mean()\n",
    "model_labels_short = [m.replace('anthropic/', '').replace('-20250929', '') for m in token_means.index]\n",
    "ax5.bar(model_labels_short, token_means.values, color=colors[:len(token_means)])\n",
    "ax5.set_xlabel('Model')\n",
    "ax5.set_ylabel('Avg Tokens')\n",
    "ax5.set_title('Avg Tokens by Model', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 6. Summary text\n",
    "ax6 = fig.add_subplot(2, 3, 6)\n",
    "ax6.axis('off')\n",
    "summary_text = f\"\"\"\n",
    "üìä SWE-bench Verified Results\n",
    "{'='*30}\n",
    "\n",
    "Total Instances: {summary['total_instances']}\n",
    "Patches Generated: {summary['total_instances'] - summary['failed_instances']}\n",
    "\n",
    "üí∞ Cost Metrics:\n",
    "  Total: ${cost['total_cost_usd']:.4f}\n",
    "  Per Instance: ${cost['cost_per_instance']:.4f}\n",
    "\n",
    "üî¢ Token Metrics:\n",
    "  Total: {tokens['total_tokens']:,}\n",
    "  Avg/Instance: {tokens['total_tokens'] // summary['total_instances']}\n",
    "\n",
    "‚è±Ô∏è Latency:\n",
    "  Mean: {df['gen_latency_seconds'].mean():.1f}s\n",
    "  Median: {df['gen_latency_seconds'].median():.1f}s\n",
    "\"\"\"\n",
    "ax6.text(0.1, 0.9, summary_text, transform=ax6.transAxes, fontsize=11,\n",
    "         verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.suptitle('SWE-bench Verified - Adaptive Router Analysis Dashboard', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('summary_dashboard.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Export Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "summary_report = {\n",
    "    'run_info': {\n",
    "        'model_name': data['model_name'],\n",
    "        'dataset': data['dataset'],\n",
    "        'timestamp': data['timestamp'],\n",
    "        'run_id': 'adaptive_20251215_124758'\n",
    "    },\n",
    "    'instance_summary': {\n",
    "        'total': summary['total_instances'],\n",
    "        'patches_generated': summary['total_instances'] - summary['failed_instances'] - summary['error_instances'],\n",
    "        'failed': summary['failed_instances'],\n",
    "        'errors': summary['error_instances']\n",
    "    },\n",
    "    'cost_summary': {\n",
    "        'total_usd': round(cost['total_cost_usd'], 4),\n",
    "        'per_instance_usd': round(cost['cost_per_instance'], 4),\n",
    "        'mean_usd': round(df['gen_cost_usd'].mean(), 4),\n",
    "        'median_usd': round(df['gen_cost_usd'].median(), 4)\n",
    "    },\n",
    "    'token_summary': {\n",
    "        'total': tokens['total_tokens'],\n",
    "        'input': tokens['total_input_tokens'],\n",
    "        'output': tokens['total_output_tokens'],\n",
    "        'avg_per_instance': tokens['total_tokens'] // summary['total_instances']\n",
    "    },\n",
    "    'latency_summary': {\n",
    "        'mean_seconds': round(df['gen_latency_seconds'].mean(), 2),\n",
    "        'median_seconds': round(df['gen_latency_seconds'].median(), 2),\n",
    "        'min_seconds': round(df['gen_latency_seconds'].min(), 2),\n",
    "        'max_seconds': round(df['gen_latency_seconds'].max(), 2)\n",
    "    },\n",
    "    'model_selection': {\n",
    "        model.replace('anthropic/', ''): {\n",
    "            'count': int(count),\n",
    "            'percentage': round(count / len(df) * 100, 1),\n",
    "            'total_cost': round(df[df['gen_model_used'] == model]['gen_cost_usd'].sum(), 4)\n",
    "        }\n",
    "        for model, count in model_counts.items()\n",
    "    },\n",
    "    'repository_distribution': {\n",
    "        repo: int(count) for repo, count in repo_counts.items()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "with open('analysis_summary.json', 'w') as f:\n",
    "    json.dump(summary_report, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ Analysis complete!\")\n",
    "print(\"\\nFiles generated:\")\n",
    "print(\"  - model_selection_distribution.png\")\n",
    "print(\"  - cost_analysis.png\")\n",
    "print(\"  - token_analysis.png\")\n",
    "print(\"  - latency_analysis.png\")\n",
    "print(\"  - repository_distribution.png\")\n",
    "print(\"  - model_selection_by_repo.png\")\n",
    "print(\"  - summary_dashboard.png\")\n",
    "print(\"  - analysis_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
