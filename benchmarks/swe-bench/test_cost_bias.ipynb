{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive API - Cost Bias Testing\n",
    "\n",
    "Test how different `cost_bias` values affect model selection between Claude Opus 4.5 and Claude Sonnet 4.5.\n",
    "\n",
    "**Cost Bias Values:**\n",
    "- `0.0` = Quality focused (prefer more capable models)\n",
    "- `0.5` = Balanced\n",
    "- `1.0` = Cost focused (prefer cheaper models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize client\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"ADAPTIVE_API_KEY\"),\n",
    "    base_url=os.getenv(\"ADAPTIVE_API_BASE\", \"https://api.llmadaptive.uk/v1/\")\n",
    ")\n",
    "\n",
    "# Models to route between\n",
    "MODELS = [\"anthropic/claude-opus-4-5\", \"anthropic/claude-sonnet-4-5\"]\n",
    "\n",
    "print(\"Adaptive API Configuration\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"API Base: {client.base_url}\")\n",
    "print(f\"Models: {MODELS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Test Prompts\n",
    "\n",
    "Different complexity levels to see how routing changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts of varying complexity\n",
    "TEST_PROMPTS = [\n",
    "    {\n",
    "        \"name\": \"simple_greeting\",\n",
    "        \"complexity\": \"low\",\n",
    "        \"prompt\": \"Say hello in 3 words.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math_question\",\n",
    "        \"complexity\": \"low\",\n",
    "        \"prompt\": \"What is 15 * 23?\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"code_simple\",\n",
    "        \"complexity\": \"medium\",\n",
    "        \"prompt\": \"Write a Python function to check if a number is prime.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"code_medium\",\n",
    "        \"complexity\": \"medium\",\n",
    "        \"prompt\": \"Write a Python function to implement binary search on a sorted list. Include docstring and type hints.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"code_complex\",\n",
    "        \"complexity\": \"high\",\n",
    "        \"prompt\": \"\"\"Design and implement a Python class for a thread-safe LRU cache with the following requirements:\n",
    "1. O(1) get and put operations\n",
    "2. Thread-safe for concurrent access\n",
    "3. Configurable max size\n",
    "4. Optional TTL (time-to-live) for entries\n",
    "Include comprehensive docstrings, type hints, and example usage.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"reasoning\",\n",
    "        \"complexity\": \"high\",\n",
    "        \"prompt\": \"\"\"Analyze the trade-offs between microservices and monolithic architecture for a startup building a social media platform. \n",
    "Consider: team size (5 developers), expected scale (100K users in year 1), budget constraints, and time-to-market pressure.\n",
    "Provide a concrete recommendation with justification.\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Defined {len(TEST_PROMPTS)} test prompts:\")\n",
    "for p in TEST_PROMPTS:\n",
    "    print(f\"  - {p['name']} ({p['complexity']} complexity)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Function for API Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_adaptive_api(prompt: str, cost_bias: float, max_tokens: int = 1024) -> dict:\n",
    "    \"\"\"\n",
    "    Call Adaptive API with specified cost_bias.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The user prompt\n",
    "        cost_bias: 0.0 (quality) to 1.0 (cost)\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with response details\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"adaptive/auto\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            extra_body={\n",
    "                \"model_router\": {\n",
    "                    \"models\": MODELS,\n",
    "                    \"cost_bias\": cost_bias\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        # Extract model used from response\n",
    "        model_used = response.model\n",
    "        \n",
    "        # Extract usage\n",
    "        usage = response.usage\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"model_used\": model_used,\n",
    "            \"input_tokens\": usage.prompt_tokens if usage else 0,\n",
    "            \"output_tokens\": usage.completion_tokens if usage else 0,\n",
    "            \"total_tokens\": usage.total_tokens if usage else 0,\n",
    "            \"latency_seconds\": latency,\n",
    "            \"content\": response.choices[0].message.content[:200] + \"...\" if len(response.choices[0].message.content) > 200 else response.choices[0].message.content,\n",
    "            \"error\": None\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"model_used\": \"error\",\n",
    "            \"input_tokens\": 0,\n",
    "            \"output_tokens\": 0,\n",
    "            \"total_tokens\": 0,\n",
    "            \"latency_seconds\": time.time() - start_time,\n",
    "            \"content\": \"\",\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "print(\"Helper function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Tests with Different Cost Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost bias values to test\n",
    "COST_BIASES = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "print(\"Running tests...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt_info in TEST_PROMPTS:\n",
    "    print(f\"\\nTesting: {prompt_info['name']} ({prompt_info['complexity']} complexity)\")\n",
    "    \n",
    "    for cost_bias in COST_BIASES:\n",
    "        print(f\"  cost_bias={cost_bias}...\", end=\" \")\n",
    "        \n",
    "        result = call_adaptive_api(\n",
    "            prompt=prompt_info[\"prompt\"],\n",
    "            cost_bias=cost_bias\n",
    "        )\n",
    "        \n",
    "        result[\"prompt_name\"] = prompt_info[\"name\"]\n",
    "        result[\"prompt_complexity\"] = prompt_info[\"complexity\"]\n",
    "        result[\"cost_bias\"] = cost_bias\n",
    "        results.append(result)\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            model_short = result[\"model_used\"].replace(\"anthropic/\", \"\").replace(\"-20250929\", \"\")\n",
    "            print(f\"-> {model_short} ({result['latency_seconds']:.2f}s)\")\n",
    "        else:\n",
    "            print(f\"-> ERROR: {result['error']}\")\n",
    "        \n",
    "        # Small delay to avoid rate limiting\n",
    "        time.sleep(0.5)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Completed {len(results)} API calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Clean model names\n",
    "df['model_short'] = df['model_used'].apply(\n",
    "    lambda x: x.replace('anthropic/', '').replace('-20250929', '') if x != 'error' else 'error'\n",
    ")\n",
    "\n",
    "print(\"Results Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(df[['prompt_name', 'cost_bias', 'model_short', 'latency_seconds', 'total_tokens']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Selection by Cost Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table: model selection by cost_bias\n",
    "selection_pivot = pd.crosstab(df['cost_bias'], df['model_short'], normalize='index') * 100\n",
    "\n",
    "print(\"\\nModel Selection % by Cost Bias:\")\n",
    "print(\"-\" * 40)\n",
    "print(selection_pivot.round(1).to_string())\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "selection_pivot.plot(kind='bar', ax=ax, color=['#e74c3c', '#3498db'])\n",
    "ax.set_xlabel('Cost Bias')\n",
    "ax.set_ylabel('Selection %')\n",
    "ax.set_title('Model Selection by Cost Bias', fontsize=14, fontweight='bold')\n",
    "ax.set_xticklabels([f'{b}' for b in COST_BIASES], rotation=0)\n",
    "ax.legend(title='Model')\n",
    "ax.set_ylim(0, 105)\n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.0f%%', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cost_bias_selection.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Selection by Prompt Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table: model selection by complexity and cost_bias\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "complexities = ['low', 'medium', 'high']\n",
    "colors = ['#3498db', '#e74c3c']\n",
    "\n",
    "for i, complexity in enumerate(complexities):\n",
    "    subset = df[df['prompt_complexity'] == complexity]\n",
    "    if len(subset) > 0:\n",
    "        pivot = pd.crosstab(subset['cost_bias'], subset['model_short'], normalize='index') * 100\n",
    "        pivot.plot(kind='bar', ax=axes[i], color=colors, legend=(i == 2))\n",
    "        axes[i].set_title(f'{complexity.capitalize()} Complexity', fontsize=12, fontweight='bold')\n",
    "        axes[i].set_xlabel('Cost Bias')\n",
    "        axes[i].set_ylabel('Selection %' if i == 0 else '')\n",
    "        axes[i].set_xticklabels([f'{b}' for b in pivot.index], rotation=0)\n",
    "        axes[i].set_ylim(0, 105)\n",
    "\n",
    "if axes[2].get_legend():\n",
    "    axes[2].legend(title='Model', bbox_to_anchor=(1.02, 1))\n",
    "\n",
    "plt.suptitle('Model Selection by Complexity and Cost Bias', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('cost_bias_by_complexity.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Latency Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latency by model\n",
    "print(\"\\nLatency by Model:\")\n",
    "print(\"-\" * 40)\n",
    "latency_stats = df.groupby('model_short')['latency_seconds'].agg(['mean', 'median', 'std', 'count'])\n",
    "print(latency_stats.round(2).to_string())\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Box plot\n",
    "models = df['model_short'].unique()\n",
    "data_for_box = [df[df['model_short'] == m]['latency_seconds'] for m in models if m != 'error']\n",
    "labels = [m for m in models if m != 'error']\n",
    "bp = axes[0].boxplot(data_for_box, labels=labels, patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].set_ylabel('Latency (seconds)')\n",
    "axes[0].set_title('Latency Distribution by Model', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Latency by cost_bias\n",
    "latency_by_bias = df.groupby('cost_bias')['latency_seconds'].mean()\n",
    "axes[1].plot(latency_by_bias.index, latency_by_bias.values, 'o-', markersize=10, linewidth=2)\n",
    "axes[1].set_xlabel('Cost Bias')\n",
    "axes[1].set_ylabel('Average Latency (seconds)')\n",
    "axes[1].set_title('Average Latency by Cost Bias', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xticks(COST_BIASES)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('latency_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Detailed Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed pivot table\n",
    "detail_table = df.pivot_table(\n",
    "    index='prompt_name',\n",
    "    columns='cost_bias',\n",
    "    values='model_short',\n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "print(\"\\nModel Selected by Prompt and Cost Bias:\")\n",
    "print(\"=\" * 80)\n",
    "print(detail_table.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics\n",
    "successful = df[df['success'] == True]\n",
    "\n",
    "opus_count = len(successful[successful['model_short'].str.contains('opus', case=False)])\n",
    "sonnet_count = len(successful[successful['model_short'].str.contains('sonnet', case=False)])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal API Calls: {len(df)}\")\n",
    "print(f\"Successful: {len(successful)}\")\n",
    "print(f\"Failed: {len(df) - len(successful)}\")\n",
    "\n",
    "print(f\"\\nModel Distribution:\")\n",
    "print(f\"  Claude Opus 4.5:   {opus_count} ({opus_count/len(successful)*100:.1f}%)\")\n",
    "print(f\"  Claude Sonnet 4.5: {sonnet_count} ({sonnet_count/len(successful)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nAverage Latency: {successful['latency_seconds'].mean():.2f}s\")\n",
    "print(f\"Average Tokens:  {successful['total_tokens'].mean():.0f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\"\"\n",
    "Based on the test results:\n",
    "\n",
    "1. cost_bias=0.0: Use for critical/complex tasks requiring best quality\n",
    "   - More likely to select Claude Opus 4.5\n",
    "   - Higher cost but better reasoning\n",
    "\n",
    "2. cost_bias=0.5: Balanced approach for general use\n",
    "   - Router intelligently selects based on task complexity\n",
    "   - Good trade-off between cost and quality\n",
    "\n",
    "3. cost_bias=1.0: Use for high-volume, simpler tasks\n",
    "   - More likely to select Claude Sonnet 4.5\n",
    "   - Lower cost, still good quality for routine tasks\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to JSON\n",
    "export_data = {\n",
    "    \"test_config\": {\n",
    "        \"models\": MODELS,\n",
    "        \"cost_biases_tested\": COST_BIASES,\n",
    "        \"prompts_tested\": len(TEST_PROMPTS)\n",
    "    },\n",
    "    \"summary\": {\n",
    "        \"total_calls\": len(df),\n",
    "        \"successful_calls\": len(successful),\n",
    "        \"opus_selections\": opus_count,\n",
    "        \"sonnet_selections\": sonnet_count,\n",
    "        \"avg_latency\": round(successful['latency_seconds'].mean(), 2),\n",
    "        \"avg_tokens\": round(successful['total_tokens'].mean(), 0)\n",
    "    },\n",
    "    \"results\": df.to_dict(orient='records')\n",
    "}\n",
    "\n",
    "with open('cost_bias_test_results.json', 'w') as f:\n",
    "    json.dump(export_data, f, indent=2, default=str)\n",
    "\n",
    "# Export to CSV\n",
    "df.to_csv('cost_bias_test_results.csv', index=False)\n",
    "\n",
    "print(\"\\nResults exported:\")\n",
    "print(\"  - cost_bias_test_results.json\")\n",
    "print(\"  - cost_bias_test_results.csv\")\n",
    "print(\"  - cost_bias_selection.png\")\n",
    "print(\"  - cost_bias_by_complexity.png\")\n",
    "print(\"  - latency_comparison.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
